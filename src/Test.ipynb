{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import grad\n",
    "from nash_advreg import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/winequality-white.csv\", sep = \";\")\n",
    "X = data.loc[:, data.columns != \"quality\"]\n",
    "y = data.quality\n",
    "##\n",
    "pca = PCA(n_components=X.shape[1], svd_solver='full')\n",
    "pca.fit(X)\n",
    "X = pca.fit_transform(X)\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MEAN = 0.01\n",
    "VAR = 0.5\n",
    "m = torch.distributions.Gamma(torch.tensor([MEAN**2/VAR]), torch.tensor([MEAN/VAR])) ## shape, rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = create_train_test(X,y)\n",
    "params = {\n",
    "                \"epochs_rr\"    : 1000,\n",
    "                \"lr_rr\"        : 0.01,\n",
    "                \"lmb\"          : 0.0,\n",
    "                \"c_d_train\"    : torch.ones([len(y_train), 1]) * MEAN,\n",
    "                \"z_train\"      : torch.zeros([len(y_train),1]),#.to(\"cuda\"),\n",
    "                \"c_d_test\"     : torch.ones([len(y_test), 1]) * MEAN,\n",
    "                \"z_test\"       : torch.zeros([len(y_test),1]),#.to(\"cuda\"),\n",
    "                \"outer_lr\"     : 10e-6,\n",
    "                \"inner_lr\"     : 0.0001,\n",
    "                \"outer_epochs\" : 20,\n",
    "                \"inner_epochs\" : 10,\n",
    "                \"n_samples\"    : 10,\n",
    "                \"prior\"        : m  \n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_d_train = params[\"prior\"].sample(torch.Size([params[\"n_samples\"], len(y_train)]))#.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 weights tensor([[-1.2256, -0.2707, -0.9159,  0.3077, -0.1688, -0.3287,  0.1883,  1.2332,\n",
      "         -0.8563,  1.3693,  1.2376,  0.1815]], grad_fn=<SubBackward0>)\n",
      "epoch 1 weights tensor([[-1.1457, -0.2518, -0.8521,  0.2901, -0.1563, -0.3045,  0.1719,  1.1455,\n",
      "         -0.7988,  1.2729,  1.1543,  0.5722]], grad_fn=<SubBackward0>)\n",
      "epoch 2 weights tensor([[-1.0692, -0.2330, -0.7995,  0.2692, -0.1436, -0.2809,  0.1586,  1.0705,\n",
      "         -0.7444,  1.1847,  1.0768,  0.9345]], grad_fn=<SubBackward0>)\n",
      "epoch 3 weights tensor([[-0.9976, -0.2186, -0.7430,  0.2530, -0.1304, -0.2563,  0.1503,  0.9967,\n",
      "         -0.6931,  1.1020,  1.0066,  1.2723]], grad_fn=<SubBackward0>)\n",
      "epoch 4 weights tensor([[-0.9288, -0.2047, -0.6934,  0.2316, -0.1176, -0.2389,  0.1442,  0.9264,\n",
      "         -0.6484,  1.0286,  0.9361,  1.5878]], grad_fn=<SubBackward0>)\n",
      "epoch 5 weights tensor([[-0.8676, -0.1897, -0.6472,  0.2145, -0.1127, -0.2227,  0.1356,  0.8612,\n",
      "         -0.6079,  0.9572,  0.8691,  1.8816]], grad_fn=<SubBackward0>)\n",
      "epoch 6 weights tensor([[-0.8062, -0.1763, -0.6036,  0.1985, -0.1046, -0.2090,  0.1287,  0.8032,\n",
      "         -0.5672,  0.8913,  0.8076,  2.1556]], grad_fn=<SubBackward0>)\n",
      "epoch 7 weights tensor([[-0.7512, -0.1672, -0.5639,  0.1869, -0.0965, -0.1948,  0.1206,  0.7500,\n",
      "         -0.5269,  0.8331,  0.7515,  2.4103]], grad_fn=<SubBackward0>)\n",
      "epoch 8 weights tensor([[-0.6984, -0.1545, -0.5281,  0.1792, -0.0878, -0.1823,  0.1100,  0.6994,\n",
      "         -0.4917,  0.7765,  0.7001,  2.6468]], grad_fn=<SubBackward0>)\n",
      "epoch 9 weights tensor([[-0.6520, -0.1435, -0.4942,  0.1652, -0.0835, -0.1672,  0.1026,  0.6537,\n",
      "         -0.4602,  0.7216,  0.6510,  2.8670]], grad_fn=<SubBackward0>)\n",
      "epoch 10 weights tensor([[-0.6071, -0.1362, -0.4583,  0.1535, -0.0782, -0.1530,  0.0938,  0.6073,\n",
      "         -0.4283,  0.6717,  0.6080,  3.0732]], grad_fn=<SubBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-704b916483f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw_bayes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_bayes_rr_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_d_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/GM_SG/src/nash_advreg.py\u001b[0m in \u001b[0;36mtrain_bayes_rr_test\u001b[0;34m(X_clean, y, c_d_train, params, verbose)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mc_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_d_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_backward_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_clean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m             \u001b[0mgrad\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GM_SG/src/nash_advreg.py\u001b[0m in \u001b[0;36mcompute_backward_derivative\u001b[0;34m(X_clean, y, w, c_d, params)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mX_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mgrad_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mgm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_tmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_tmp\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0milr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrad_X\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;31m## To compute Hessian Vector Product\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0maux1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w_bayes = train_bayes_rr_test(X_train, y_train, c_d_train, params, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#torch.max(c_d_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_nash_rr_test(X_train, y_train, c_d_train, params, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_clean = X_train\n",
    "#y = y_train\n",
    "#w = torch.randn(1, X_clean.shape[1] + 1, requires_grad=True)\n",
    "#c_d = torch.ones([len(y_train), 1]) * 1.0\n",
    "#compute_backwar_derivative(X_clean, y, w, c_d, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 weights tensor([[-1.1243, -1.5460,  0.6223, -0.1238, -0.8885, -0.2759,  0.3897, -1.1944,\n",
      "         -0.7417,  2.1541, -0.0957,  0.6892]], grad_fn=<SubBackward0>)\n",
      "epoch 1 weights tensor([[-1.1083, -1.5193,  0.6112, -0.1194, -0.8737, -0.2718,  0.3832, -1.1750,\n",
      "         -0.7316,  2.1192, -0.0954,  0.8772]], grad_fn=<SubBackward0>)\n",
      "epoch 2 weights tensor([[-1.0888, -1.4886,  0.5989, -0.1148, -0.8556, -0.2668,  0.3752, -1.1521,\n",
      "         -0.7182,  2.0780, -0.0945,  1.0635]], grad_fn=<SubBackward0>)\n",
      "epoch 3 weights tensor([[-1.0665, -1.4533,  0.5836, -0.1103, -0.8371, -0.2606,  0.3676, -1.1252,\n",
      "         -0.7045,  2.0326, -0.0917,  1.2473]], grad_fn=<SubBackward0>)\n",
      "epoch 4 weights tensor([[-1.0406, -1.4136,  0.5705, -0.1052, -0.8153, -0.2540,  0.3605, -1.0976,\n",
      "         -0.6881,  1.9798, -0.0892,  1.4311]], grad_fn=<SubBackward0>)\n",
      "epoch 5 weights tensor([[-1.0128, -1.3692,  0.5525, -0.1000, -0.7917, -0.2461,  0.3500, -1.0667,\n",
      "         -0.6699,  1.9200, -0.0878,  1.6130]], grad_fn=<SubBackward0>)\n",
      "epoch 6 weights tensor([[-0.9803, -1.3221,  0.5337, -0.0940, -0.7645, -0.2365,  0.3375, -1.0306,\n",
      "         -0.6474,  1.8543, -0.0858,  1.7946]], grad_fn=<SubBackward0>)\n",
      "epoch 7 weights tensor([[-0.9432, -1.2711,  0.5132, -0.0850, -0.7343, -0.2259,  0.3255, -0.9917,\n",
      "         -0.6221,  1.7836, -0.0848,  1.9752]], grad_fn=<SubBackward0>)\n",
      "epoch 8 weights tensor([[-0.9039, -1.2131,  0.4900, -0.0778, -0.7012, -0.2135,  0.3124, -0.9489,\n",
      "         -0.5962,  1.7055, -0.0811,  2.1551]], grad_fn=<SubBackward0>)\n",
      "epoch 9 weights tensor([[-0.8600, -1.1509,  0.4662, -0.0718, -0.6652, -0.2011,  0.2976, -0.9022,\n",
      "         -0.5673,  1.6197, -0.0782,  2.3355]], grad_fn=<SubBackward0>)\n",
      "epoch 10 weights tensor([[-0.8113, -1.0867,  0.4382, -0.0647, -0.6269, -0.1888,  0.2795, -0.8520,\n",
      "         -0.5333,  1.5288, -0.0743,  2.5150]], grad_fn=<SubBackward0>)\n",
      "epoch 11 weights tensor([[-0.7602, -1.0153,  0.4086, -0.0587, -0.5863, -0.1750,  0.2637, -0.7979,\n",
      "         -0.5016,  1.4286, -0.0697,  2.6950]], grad_fn=<SubBackward0>)\n",
      "epoch 12 weights tensor([[-0.7071, -0.9388,  0.3772, -0.0502, -0.5438, -0.1621,  0.2453, -0.7385,\n",
      "         -0.4658,  1.3233, -0.0661,  2.8743]], grad_fn=<SubBackward0>)\n",
      "epoch 13 weights tensor([[-0.6495, -0.8595,  0.3454, -0.0418, -0.4978, -0.1470,  0.2240, -0.6784,\n",
      "         -0.4278,  1.2113, -0.0621,  3.0526]], grad_fn=<SubBackward0>)\n",
      "epoch 14 weights tensor([[-0.5906, -0.7774,  0.3117, -0.0324, -0.4516, -0.1330,  0.2031, -0.6142,\n",
      "         -0.3892,  1.0969, -0.0570,  3.2287]], grad_fn=<SubBackward0>)\n",
      "epoch 15 weights tensor([[-0.5299, -0.6920,  0.2793, -0.0262, -0.4060, -0.1180,  0.1816, -0.5489,\n",
      "         -0.3496,  0.9824, -0.0527,  3.4014]], grad_fn=<SubBackward0>)\n",
      "epoch 16 weights tensor([[-0.4712, -0.6091,  0.2461, -0.0197, -0.3583, -0.1032,  0.1627, -0.4850,\n",
      "         -0.3110,  0.8684, -0.0481,  3.5690]], grad_fn=<SubBackward0>)\n",
      "epoch 17 weights tensor([[-0.4142, -0.5325,  0.2142, -0.0133, -0.3128, -0.0884,  0.1430, -0.4248,\n",
      "         -0.2740,  0.7593, -0.0439,  3.7303]], grad_fn=<SubBackward0>)\n",
      "epoch 18 weights tensor([[-0.3599, -0.4598,  0.1845, -0.0089, -0.2703, -0.0759,  0.1263, -0.3683,\n",
      "         -0.2402,  0.6597, -0.0399,  3.8840]], grad_fn=<SubBackward0>)\n",
      "epoch 19 weights tensor([[-3.1169e-01, -3.9513e-01,  1.5908e-01, -2.6153e-03, -2.3285e-01,\n",
      "         -6.3646e-02,  1.1104e-01, -3.1873e-01, -2.0939e-01,  5.6978e-01,\n",
      "         -3.4376e-02,  4.0285e+00]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "w_rr = train_rr(X_train, y_train, params)\n",
    "w_nash = train_nash_rr(X_train, y_train, params)\n",
    "w_bayes = train_bayes_rr_test(X_train, y_train, c_d_train, params, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_clean = X_train.to(\"cuda\")\n",
    "#y = y_train.to(\"cuda\")\n",
    "#w = torch.randn(1, X_train.shape[1] + 1, requires_grad=True)#.to(\"cuda\")\n",
    "#compute_backward_derivative(X_clean, y, w, c_d_train[0], params)\n",
    "#w_bayes = train_bayes_rr_test(X_train, y_train, c_d_train, params, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare rmse's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_d_test = params[\"prior\"].sample(torch.Size([1, len(y_test)]))[0]\n",
    "##\n",
    "X_test_attacked = attack(X_test, w_rr, c_d_test, params[\"z_test\"])\n",
    "pred_attacked =  predict(X_test_attacked, w_rr)\n",
    "pred_clean    =  predict(X_test, w_rr)\n",
    "#\n",
    "rmse_raw_clean = rmse( y_test, pred_clean )\n",
    "rmse_raw_at    = rmse( y_test, pred_attacked )\n",
    "#\n",
    "##\n",
    "X_test_attacked = attack(X_test, w_nash, c_d_test, params[\"z_test\"])\n",
    "pred_attacked =  predict(X_test_attacked, w_nash)\n",
    "pred_clean    =  predict(X_test, w_nash)\n",
    "#\n",
    "rmse_nash_clean = rmse( y_test, pred_clean )\n",
    "rmse_nash_at    = rmse( y_test, pred_attacked )\n",
    "##\n",
    "X_test_attacked = attack(X_test, w_bayes, c_d_test, params[\"z_test\"])\n",
    "pred_attacked =  predict(X_test_attacked, w_bayes)\n",
    "pred_clean    =  predict(X_test, w_bayes)\n",
    "#\n",
    "rmse_bayes_clean = rmse( y_test, pred_clean )\n",
    "rmse_bayes_at    = rmse( y_test, pred_attacked )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____Non-Strategic Defender____\n",
      "Loss Clean test set:  tensor(0.7554, grad_fn=<SqrtBackward>)\n",
      "Loss attacked test set:  tensor(1.0300, grad_fn=<SqrtBackward>)\n",
      "\n",
      "____Strategic Bayes Defender____\n",
      "Loss Bayes Clean test set:  tensor(2.1978, grad_fn=<SqrtBackward>)\n",
      "Loss Bayes attacked test set:  tensor(3.0708, grad_fn=<SqrtBackward>)\n",
      "\n",
      "____Strategic Nash Defender____\n",
      "Loss Nash Clean test set:  tensor(5.9693, grad_fn=<SqrtBackward>)\n",
      "Loss Nash attacked test set:  tensor(5.7955, grad_fn=<SqrtBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(\"____Non-Strategic Defender____\")\n",
    "###\n",
    "print( \"Loss Clean test set: \", rmse_raw_clean )\n",
    "###\n",
    "print( \"Loss attacked test set: \", rmse_raw_at )\n",
    "###\n",
    "print(\"\\n____Strategic Bayes Defender____\")\n",
    "###\n",
    "print( \"Loss Bayes Clean test set: \", rmse_bayes_clean )\n",
    "###\n",
    "print( \"Loss Bayes attacked test set: \", rmse_bayes_at )\n",
    "###\n",
    "print(\"\\n____Strategic Nash Defender____\")\n",
    "###\n",
    "print( \"Loss Nash Clean test set: \", rmse_nash_clean )\n",
    "###\n",
    "print( \"Loss Nash attacked test set: \", rmse_nash_at )\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "tensor([[-0.9592,  0.4510,  0.0282,  ...,  0.9187, -0.0400,  0.3336],\n",
      "        [-0.9283,  0.4420,  0.0763,  ...,  0.8839, -0.0714,  0.3110],\n",
      "        [-0.8979,  0.4352,  0.1225,  ...,  0.8542, -0.0977,  0.2921],\n",
      "        ...,\n",
      "        [ 0.5680,  0.5747,  2.1145,  ...,  0.3939, -0.3506,  0.0751],\n",
      "        [ 0.5686,  0.5747,  2.1153,  ...,  0.3937, -0.3506,  0.0751],\n",
      "        [ 0.5692,  0.5748,  2.1161,  ...,  0.3936, -0.3507,  0.0750]],\n",
      "       device='cuda:0', grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "lr = params[\"outer_lr\"]\n",
    "T = params[\"outer_epochs\"]\n",
    "n_samples = params[\"n_samples\"]\n",
    "S = params[\"inner_epochs\"]\n",
    "ilr = params[\"inner_lr\"]\n",
    "X = torch.randn(X_clean.shape[0]*X_clean.shape[1], requires_grad=True).to(\"cuda\")\n",
    "##\n",
    "Xt = torch.zeros(int(S), X.shape[0]).to(\"cuda\")\n",
    "\n",
    "print(Xt)\n",
    "c_d = c_d_train[0]\n",
    "gr = compute_backward_derivative(X_clean, X, Xt, y, w, c_d, S, ilr, params)\n",
    "gr\n",
    "print(Xt)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.48 s, sys: 88 ms, total: 1.57 s\n",
      "Wall time: 1.56 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[   70.1343, -1483.9263,   692.1609,  -201.3738,   850.2290,   -58.0689,\n",
       "           -49.1796,   220.8937,   802.0889,   718.0339,   547.9590, -4548.6367]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "compute_backward_derivative(X_clean, X, Xt, y, w, c_d, S, ilr, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.29 s, sys: 36 ms, total: 1.33 s\n",
      "Wall time: 1.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  991.3489,   467.4733,  -234.6497,  -610.4382,   644.6360,    -6.5245,\n",
       "           380.2441,   386.2027,    54.4584,   218.5261,  -343.6595, -4406.8594]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "compute_backward_derivative(X_clean, y, w, c_d_train[0], params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_clean = X_train\n",
    "y = y_train\n",
    "##\n",
    "lr = params[\"outer_lr\"]\n",
    "T = params[\"outer_epochs\"]\n",
    "n_samples = params[\"n_samples\"]\n",
    "w = torch.randn(1, X_clean.shape[1] + 1, requires_grad=True)\n",
    "\n",
    "#X = torch.randn(X_clean.shape[0]*X_clean.shape[1], requires_grad=True)\n",
    "#fm = lambda w, X: learner_cost_flatten(X, y, w, params)\n",
    "\n",
    "\n",
    "grad = torch.zeros(1, X_clean.shape[1] + 1)\n",
    "c_d = c_d_train[0].t()[0]\n",
    "grad = compute_backwar_derivative(X_clean, y, w, c_d, params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
