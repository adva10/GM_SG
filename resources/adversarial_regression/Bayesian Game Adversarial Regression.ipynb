{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import grad\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def redim(x):\n",
    "    return(np.append(x, np.ones([x.shape[0],1]), axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stand(x, mean, std):\n",
    "    x = x - mean\n",
    "    x = x/std \n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        6  \n",
       "1      9.5        6  \n",
       "2     10.1        6  \n",
       "3      9.9        6  \n",
       "4      9.9        6  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/winequality-white.csv\", sep = \";\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data.loc[:, data.columns != \"quality\"]\n",
    "y = data.quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract first few PCA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.09657344e-01 7.93338631e-02 1.01542742e-02 5.06004450e-04\n",
      " 3.23409395e-04 8.72769740e-06 6.72986618e-06 5.39060918e-06\n",
      " 4.07002123e-06 1.86525322e-07 1.49217279e-10]\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=11, svd_solver='full')\n",
    "pca.fit(X)                 \n",
    "print(pca.explained_variance_ratio_) \n",
    "x = pca.fit_transform(X)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
    "x_train = np.asarray(x_train,dtype=np.float32)\n",
    "y_train = np.asarray(y_train,dtype=np.float32).reshape(-1,1)\n",
    "x_test = np.asarray(x_test,dtype=np.float32) #un poco trampa\n",
    "y_test = np.asarray(y_test,dtype=np.float32).reshape(-1,1)\n",
    "### to torch\n",
    "x_train = Variable( torch.from_numpy(x_train) )\n",
    "y_train = Variable( torch.from_numpy(y_train) )\n",
    "x_test = torch.from_numpy(x_test) \n",
    "y_test = torch.from_numpy(y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_renom = stand(x_train, x_train.mean(dim=0), x_train.std(dim=0))\n",
    "x_test_renom = stand(x_test, x_train.mean(dim=0), x_train.std(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = torch.randn(1, x_train.shape[1], requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "lmb = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(x, w, b):\n",
    "    return( x @ w.t() + b )\n",
    "\n",
    "def mse(t1, t2, w):\n",
    "    diff = t1 - t2\n",
    "    return( torch.sum(diff * diff) / diff.numel() + lmb*w @ w.t() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1000, loss tensor([0.5823])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    epoch += 1\n",
    "    preds = model(x_train_renom, w, b)\n",
    "    loss = mse(preds, y_train, w)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * lr\n",
    "        b -= b.grad * lr\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "        \n",
    "    if epoch%1000== 0:\n",
    "        print('epoch {}, loss {}'.format(epoch,loss.data[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1357,  0.1397, -0.0269,  0.3645,  0.0027,  0.0628,  0.0802,  0.0006,\n",
      "         -0.1693, -0.0171, -0.0891]], requires_grad=True)\n",
      "tensor([5.8926], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7213, grad_fn=<SqrtBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(x_test_renom, w, b)\n",
    "torch.sqrt( torch.mean( (preds - y_test )**2 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse(y, pred):\n",
    "    return torch.sqrt( torch.mean( (pred - y )**2 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7213, grad_fn=<SqrtBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial attack. Non-Bayesian case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X$ denote the clean dataset, and $X^* = T(X, \\beta)$ the attacked dataset, when the classifier choose parameters $\\beta$. We try to solve the following Defend-Attack game\n",
    "\n",
    "$$\n",
    "\\beta^* = \\arg\\min_{\\beta} \\widehat{\\theta}_C [\\beta, T(X, \\beta)] = \\arg\\min_{\\beta} \\sum_{i=1}^n \\left( T(x, \\beta)^{[i]}\\beta^{\\top} - y_i \\right)^2 + \\lambda \\beta \\beta^{\\top}\n",
    "$$\n",
    "\n",
    "subject to\n",
    "\n",
    "$$\n",
    "X^* = T(X, \\beta) = \\arg\\min_{X'} \\widehat{\\theta}_A [\\beta, X'] = \\arg\\min_{X'} \\sum_{i=1}^n c_{i}\\left( X'^{[i]}\\beta^{\\top} - z_i \\right)^2 + ||X-X'||^2_{F}\n",
    "$$\n",
    "\n",
    "Where $y$ are the true labels, $z$ are the targets and $c$ are instance-specific factors, which are common knowledge here. We can solve exactly the attacker's problem, yielding\n",
    "\n",
    "$$\n",
    "X^* = T(X, \\beta) = X - \\left(\\text{diag}(c_d)^{-1} + \\beta \\beta^{\\top} I_n \\right)^{-1} (X\\beta - z)\\beta^\\top\n",
    "$$\n",
    "\n",
    "We could then compute the gradient for the classifier problem using\n",
    "\n",
    "$$\n",
    "\\nabla \\widehat{\\theta}_C [\\beta, T(X, \\beta)] = \\nabla_{\\beta} \\widehat{\\theta}_C [\\beta, T(X, \\beta)] + \\nabla_T  \\widehat{\\theta}_C [\\beta, T(X, \\beta)] \\frac{\\partial T(X,\\beta)}{\\partial \\beta}\n",
    "$$\n",
    "\n",
    "and use gradient descent to find $\\beta^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attack(w, b, test, c_d, z):\n",
    "    c_d = ( 1/c_d + w @ w.t() )**(-1)\n",
    "    p1 = torch.diag( c_d[0] )\n",
    "    #p1 = torch.inverse( torch.inverse( torch.diag(c_d) ) +  w @ w.t() * torch.eye( test.shape[0] ) )\n",
    "    p2 = ( test @ w.t() + b - z)@w \n",
    "    out = test - p1 @ p2\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "value = 0.5 ## Same c_i for every instance\n",
    "c_d = torch.ones(len(y_test))*value\n",
    "z = torch.zeros([len(y_test),1]) \n",
    "out = attack(w, b, x_test_renom, c_d, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1357,  0.1397, -0.0269,  0.3645,  0.0027,  0.0628,  0.0802,  0.0006,\n",
      "         -0.1693, -0.0171, -0.0891]], requires_grad=True)\n",
      "Clean test RMSE:  tensor(0.7213, grad_fn=<SqrtBackward>)\n",
      "Attacked est RMSE:  tensor(0.9126, grad_fn=<SqrtBackward>)\n"
     ]
    }
   ],
   "source": [
    "pred_at =  model(out, w, b)\n",
    "pred_clean = model(x_test_renom, w, b)\n",
    "print(w)\n",
    "print(\"Clean test RMSE: \", torch.sqrt( torch.mean( (pred_clean - y_test )**2 ) ) )\n",
    "print(\"Attacked est RMSE: \", torch.sqrt( torch.mean( (pred_at- y_test )**2 ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Defense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_clean = torch.randn(1, x_train.shape[1], requires_grad=True)\n",
    "b_clean = torch.randn(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learner_cost(w, b, x, y, lmb, c_d, z):\n",
    "    out = attack(w, b, x, c_d, z)\n",
    "    #out = stand(out, out.mean(dim=0), out.std(dim=0))\n",
    "    #print(out.std(dim=0))\n",
    "    return torch.sum( (out @ w.t() + b - y)**2 ) +  lmb * w @ w.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost tensor([[133705.3125]], grad_fn=<AddBackward0>)\n",
      "epoch 100, loss tensor([5444.7485])\n",
      "epoch 200, loss tensor([2060.9519])\n",
      "epoch 300, loss tensor([2015.6543])\n",
      "epoch 400, loss tensor([2004.2576])\n",
      "epoch 500, loss tensor([1999.9907])\n"
     ]
    }
   ],
   "source": [
    "lr = 10e-6\n",
    "epochs = 500\n",
    "value = 0.5\n",
    "c_d = torch.ones(len(y_train))*value\n",
    "z = torch.zeros([len(y_train),1]) \n",
    "print(\"Initial Cost\", learner_cost(w_clean, b_clean, x_train_renom, y_train, lmb, c_d, z))\n",
    "for epoch in range(epochs):\n",
    "    epoch += 1\n",
    "    loss = learner_cost(w_clean, b_clean, x_train_renom, y_train, lmb, c_d, z)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w_clean -= w_clean.grad * lr\n",
    "        b_clean -= b_clean.grad * lr\n",
    "        w_clean.grad.zero_()\n",
    "        b_clean.grad.zero_()\n",
    "        \n",
    "    if epoch%100 == 0:\n",
    "        print('epoch {}, loss {}'.format(epoch,loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7638, grad_fn=<SqrtBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_at =  model(attack(w_clean, b_clean, x_train_renom, c_d, z), w_clean, b_clean)\n",
    "torch.sqrt( torch.mean( (pred_at - y_train )**2 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1418,  0.1457, -0.0285,  0.3789,  0.0027,  0.0653,  0.0836,  0.0008,\n",
      "         -0.1758, -0.0180, -0.0924]], requires_grad=True) tensor([6.5811], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(w_clean,b_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7225, grad_fn=<SqrtBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = 0.5\n",
    "c_d = torch.ones(len(y_test))*value\n",
    "z = torch.zeros([len(y_test),1]) \n",
    "pred_at = model(attack(w_clean, b_clean, x_test_renom, c_d, z), w_clean, b_clean)\n",
    "torch.sqrt( torch.mean( (pred_at - y_test )**2 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0113, grad_fn=<SqrtBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_at = model(x_test_renom, w_clean, b_clean)\n",
    "torch.sqrt( torch.mean( (pred_at - y_test )**2 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial attack. Bayesian case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we include uncertainty about the parameters of the adversary, in particular about $c$, the instance-specific factors. We choos a prior $\\pi(c)$ reflecting the defender's uncertainty. Thus we should solve\n",
    "\n",
    "$$\n",
    "\\beta^* = \\arg\\min_{\\beta} \\int \\widehat{\\theta}_C [\\beta, T(X, \\beta, c)] d\\pi(c) \n",
    "$$\n",
    "\n",
    "subject to\n",
    "\n",
    "$$\n",
    "X^* = T(X, \\beta) = \\arg\\min_{X'} \\widehat{\\theta}_A [\\beta, X', c] = \\arg\\min_{X'} \\sum_{i=1}^n c_{i}\\left( X'^{[i]}\\beta^{\\top} - z_i \\right)^2 + ||X-X'||^2_{F}\n",
    "$$\n",
    "\n",
    "where we have made explicit the dependence on $c$. To solve this problem, we can estimate the gradient of the defender's cost function with respect to $\\beta$ as follows\n",
    "\n",
    "$$\n",
    "\\nabla \\int \\widehat{\\theta}_C [\\beta, T(X, \\beta, c)] d\\pi(c)  = \\int \\nabla \\widehat{\\theta}_C [\\beta, T(X, \\beta, c)] d\\pi(c) = \\mathbb{E}_{\\pi(c)} \\left \\lbrace \\nabla \\widehat{\\theta}_C [\\beta, T(X, \\beta, c)] \\right \\rbrace\n",
    "$$\n",
    "\n",
    "Thus, we can get an unbiased estimate of the gradient, sampling $c_1, \\dots, c_K \\sim \\pi(c)$ and computing\n",
    "\n",
    "$$\n",
    "\\frac{1}{K} \\sum_{i=1}^K \\nabla \\widehat{\\theta}_C [\\beta, T(X, \\beta, c_i)]\n",
    "$$\n",
    "\n",
    "We use here a Gamma distribution as prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_bayes = torch.randn(1, x_train.shape[1], requires_grad=True)\n",
    "b_bayes = torch.randn(1, requires_grad=True)\n",
    "###\n",
    "mean = 0.5\n",
    "var = 0.01\n",
    "m = torch.distributions.Gamma(torch.tensor([mean**2/var]), torch.tensor([mean/var])) ## shape, rate\n",
    "sample_test = m.sample(torch.Size([len(y_test)]))\n",
    "z = torch.zeros([len(y_train),1]) \n",
    "n_samples = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, loss tensor([114847.2578])\n",
      "epoch 20, loss tensor([82031.2266])\n",
      "epoch 30, loss tensor([16142.8252])\n",
      "epoch 40, loss tensor([5854.7026])\n",
      "epoch 50, loss tensor([3296.0908])\n",
      "epoch 60, loss tensor([2596.5701])\n",
      "epoch 70, loss tensor([2353.6360])\n",
      "epoch 80, loss tensor([2243.7227])\n",
      "epoch 90, loss tensor([2186.7139])\n",
      "epoch 100, loss tensor([2149.5945])\n"
     ]
    }
   ],
   "source": [
    "lr = 10e-6\n",
    "epochs = 100\n",
    "z = torch.zeros([len(y_train),1]) \n",
    "for epoch in range(epochs):\n",
    "    epoch += 1\n",
    "    wgrad = torch.zeros(1, x_train.shape[1])\n",
    "    bgrad = torch.zeros(1)\n",
    "    sample = m.sample(torch.Size([n_samples, len(y_train)]))\n",
    "    ### Forma cutre. Vectorizar\n",
    "    for i in range(n_samples):\n",
    "        c_d = sample[i].t()[0]\n",
    "        loss = learner_cost(w_bayes, b_bayes, x_train_renom, y_train, lmb, c_d, z)\n",
    "        loss.backward()\n",
    "        wgrad += w_bayes.grad\n",
    "        bgrad += b_bayes.grad\n",
    "        w_bayes.grad.zero_()\n",
    "        b_bayes.grad.zero_()\n",
    "    ####     \n",
    "    wgrad /= n_samples\n",
    "    bgrad /= n_samples\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        w_bayes -= wgrad * lr\n",
    "        b_bayes -= bgrad * lr\n",
    "\n",
    "        \n",
    "    if epoch%10 == 0:\n",
    "        print('epoch {}, loss {}'.format(epoch,loss.data[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7912, grad_fn=<SqrtBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_d = sample[0].t()[0]\n",
    "z = torch.zeros([len(y_train),1]) \n",
    "pred_at = model(attack(w_bayes, b_bayes, x_train_renom, c_d, z), w_bayes, b_bayes)\n",
    "torch.sqrt( torch.mean( (pred_at - y_train )**2 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____Non-Strategic Defender____\n",
      "Loss Clean test set:  tensor(0.7213, grad_fn=<SqrtBackward>)\n",
      "Loss attacked test set:  tensor(0.9206, grad_fn=<SqrtBackward>)\n",
      "\n",
      "____Strategic Bayes Defender____\n",
      "Loss Bayes Clean test set:  tensor(0.7726, grad_fn=<SqrtBackward>)\n",
      "Loss Bayes attacked test set:  tensor(0.7527, grad_fn=<SqrtBackward>)\n",
      "\n",
      "____Strategic Nash Defender____\n",
      "Loss Nash Clean test set:  tensor(1.0113, grad_fn=<SqrtBackward>)\n",
      "Loss Nash attacked test set:  tensor(0.7383, grad_fn=<SqrtBackward>)\n"
     ]
    }
   ],
   "source": [
    "c_d = sample_test.t()[0]\n",
    "z = torch.zeros([len(y_test),1])\n",
    "\n",
    "print(\"____Non-Strategic Defender____\")\n",
    "###\n",
    "pred_clean = model(x_test_renom, w, b)\n",
    "print( \"Loss Clean test set: \", rmse(pred_clean, y_test) )\n",
    "###\n",
    "pred_at = model(attack(w, b, x_test_renom, c_d, z), w, b)\n",
    "print( \"Loss attacked test set: \", rmse(pred_at, y_test) )\n",
    "###\n",
    "print(\"\\n____Strategic Bayes Defender____\")\n",
    "###\n",
    "pred_clean = model(x_test_renom, w_bayes, b_bayes)\n",
    "print( \"Loss Bayes Clean test set: \", rmse(pred_clean, y_test) )\n",
    "###\n",
    "pred_at = model(attack(w_bayes, b_bayes, x_test_renom, c_d, z), w_bayes, b_bayes)\n",
    "print( \"Loss Bayes attacked test set: \", rmse(pred_at, y_test) )\n",
    "###\n",
    "print(\"\\n____Strategic Nash Defender____\")\n",
    "###\n",
    "pred_clean = model(x_test_renom, w_clean, b_clean)\n",
    "print( \"Loss Nash Clean test set: \", rmse(pred_clean, y_test) )\n",
    "###\n",
    "pred_at = model(attack(w_clean, b_clean, x_test_renom, c_d, z), w_clean, b_clean)\n",
    "print( \"Loss Nash attacked test set: \", rmse(pred_at, y_test) )\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The defender's cost is\n",
    "\n",
    "$$\n",
    "\\theta_l = \\sum_{i=1}^n c_{l,i} \\left( \\phi(\\omega, x)^{[i]}\\omega^{\\top} - y_i \\right)^2 + \\omega \\omega^{\\top}\n",
    "$$\n",
    "We will need to compute\n",
    "\n",
    "$$\n",
    "\\frac{d \\theta}{d\\omega_j} = \\sum_{i=1}^n c_{l,i} \\left[ 2 \\left( \\phi(\\omega, x)^{[i]}\\omega^{\\top} - y_i \\right) \\phi^{[i,j]} + 2 \\left( \\phi(\\omega, x)^{[i]}\\omega^{\\top} - y_i \\right) \\omega \\left[ \\frac{\\partial \\phi^i}{\\partial \\omega_j}\\right]^{\\top}\\right] + 2 \\omega_j\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learner_cost(w, b, x, y, lmb):\n",
    "    return torch.sum( (x @ w.t() + b - y)**2 ) +  lmb * w @ w.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-20.6633,  -4.1001,   4.4785,   2.4156,   9.6904,  -9.3381,\n",
      "          -0.2540,  -3.7007, -21.7405,   3.4108,  -2.7130]])\n",
      "tensor([[-1.1637e-02,  1.1262e-02, -3.3636e-03,  2.8633e-02, -2.3743e-05,\n",
      "          5.5320e-03,  6.9226e-03, -1.0277e-05, -1.3710e-02, -1.3846e-03,\n",
      "         -6.7525e-03],\n",
      "        [-2.7212e-01,  2.6336e-01, -7.8657e-02,  6.6956e-01, -5.5521e-04,\n",
      "          1.2936e-01,  1.6188e-01, -2.4032e-04, -3.2059e-01, -3.2377e-02,\n",
      "         -1.5790e-01],\n",
      "        [ 7.2265e-02, -6.9939e-02,  2.0889e-02, -1.7781e-01,  1.4744e-04,\n",
      "         -3.4355e-02, -4.2990e-02,  6.3821e-05,  8.5138e-02,  8.5983e-03,\n",
      "          4.1934e-02],\n",
      "        [-8.5773e-02,  8.3013e-02, -2.4793e-02,  2.1105e-01, -1.7501e-04,\n",
      "          4.0776e-02,  5.1026e-02, -7.5751e-05, -1.0105e-01, -1.0206e-02,\n",
      "         -4.9773e-02],\n",
      "        [ 2.0279e-01, -1.9627e-01,  5.8618e-02, -4.9898e-01,  4.1376e-04,\n",
      "         -9.6407e-02, -1.2064e-01,  1.7910e-04,  2.3892e-01,  2.4129e-02,\n",
      "          1.1768e-01],\n",
      "        [ 6.4541e-02, -6.2464e-02,  1.8656e-02, -1.5881e-01,  1.3168e-04,\n",
      "         -3.0683e-02, -3.8395e-02,  5.7000e-05,  7.6038e-02,  7.6793e-03,\n",
      "          3.7452e-02],\n",
      "        [-5.3146e-02,  5.1435e-02, -1.5362e-02,  1.3077e-01, -1.0843e-04,\n",
      "          2.5265e-02,  3.1616e-02, -4.6936e-05, -6.2613e-02, -6.3235e-03,\n",
      "         -3.0839e-02],\n",
      "        [-1.9721e-01,  1.9086e-01, -5.7004e-02,  4.8524e-01, -4.0237e-04,\n",
      "          9.3752e-02,  1.1732e-01, -1.7417e-04, -2.3234e-01, -2.3465e-02,\n",
      "         -1.1444e-01],\n",
      "        [ 3.9557e-02, -3.8284e-02,  1.1434e-02, -9.7331e-02,  8.0709e-05,\n",
      "         -1.8805e-02, -2.3532e-02,  3.4935e-05,  4.6603e-02,  4.7066e-03,\n",
      "          2.2954e-02],\n",
      "        [ 2.8485e-02, -2.7568e-02,  8.2336e-03, -7.0088e-02,  5.8118e-05,\n",
      "         -1.3541e-02, -1.6945e-02,  2.5156e-05,  3.3559e-02,  3.3892e-03,\n",
      "          1.6529e-02],\n",
      "        [-1.2809e-01,  1.2396e-01, -3.7024e-02,  3.1516e-01, -2.6134e-04,\n",
      "          6.0892e-02,  7.6198e-02, -1.1312e-04, -1.5090e-01, -1.5240e-02,\n",
      "         -7.4326e-02],\n",
      "        [ 3.5085e-01, -3.3956e-01,  1.0142e-01, -8.6329e-01,  7.1586e-04,\n",
      "         -1.6680e-01, -2.0872e-01,  3.0986e-04,  4.1336e-01,  4.1746e-02,\n",
      "          2.0359e-01],\n",
      "        [ 2.3655e-01, -2.2894e-01,  6.8376e-02, -5.8204e-01,  4.8264e-04,\n",
      "         -1.1246e-01, -1.4072e-01,  2.0891e-04,  2.7869e-01,  2.8146e-02,\n",
      "          1.3727e-01],\n",
      "        [ 1.3685e-01, -1.3245e-01,  3.9558e-02, -3.3673e-01,  2.7922e-04,\n",
      "         -6.5059e-02, -8.1413e-02,  1.2086e-04,  1.6123e-01,  1.6283e-02,\n",
      "          7.9412e-02],\n",
      "        [ 4.0041e-02, -3.8752e-02,  1.1574e-02, -9.8522e-02,  8.1696e-05,\n",
      "         -1.9035e-02, -2.3820e-02,  3.5362e-05,  4.7173e-02,  4.7642e-03,\n",
      "          2.3235e-02],\n",
      "        [ 3.9948e-01, -3.8662e-01,  1.1547e-01, -9.8294e-01,  8.1507e-04,\n",
      "         -1.8991e-01, -2.3765e-01,  3.5280e-04,  4.7064e-01,  4.7531e-02,\n",
      "          2.3181e-01],\n",
      "        [-3.1003e-02,  3.0006e-02, -8.9616e-03,  7.6285e-02, -6.3257e-05,\n",
      "          1.4739e-02,  1.8444e-02, -2.7381e-05, -3.6526e-02, -3.6889e-03,\n",
      "         -1.7991e-02],\n",
      "        [ 2.3596e-01, -2.2837e-01,  6.8206e-02, -5.8060e-01,  4.8144e-04,\n",
      "         -1.1218e-01, -1.4037e-01,  2.0839e-04,  2.7800e-01,  2.8076e-02,\n",
      "          1.3692e-01],\n",
      "        [ 3.4666e-02, -3.3550e-02,  1.0020e-02, -8.5297e-02,  7.0730e-05,\n",
      "         -1.6480e-02, -2.0623e-02,  3.0615e-05,  4.0841e-02,  4.1247e-03,\n",
      "          2.0116e-02],\n",
      "        [-5.3496e-01,  5.1775e-01, -1.5463e-01,  1.3163e+00, -1.0915e-03,\n",
      "          2.5432e-01,  3.1825e-01, -4.7246e-04, -6.3026e-01, -6.3652e-02,\n",
      "         -3.1043e-01],\n",
      "        [ 7.9762e-02, -7.7195e-02,  2.3056e-02, -1.9626e-01,  1.6274e-04,\n",
      "         -3.7919e-02, -4.7450e-02,  7.0442e-05,  9.3971e-02,  9.4903e-03,\n",
      "          4.6284e-02],\n",
      "        [ 2.0628e-01, -1.9964e-01,  5.9627e-02, -5.0757e-01,  4.2088e-04,\n",
      "         -9.8066e-02, -1.2272e-01,  1.8218e-04,  2.4303e-01,  2.4544e-02,\n",
      "          1.1970e-01],\n",
      "        [-1.8724e-01,  1.8121e-01, -5.4121e-02,  4.6070e-01, -3.8202e-04,\n",
      "          8.9011e-02,  1.1139e-01, -1.6536e-04, -2.2059e-01, -2.2278e-02,\n",
      "         -1.0865e-01],\n",
      "        [-2.3997e-01,  2.3225e-01, -6.9364e-02,  5.9045e-01, -4.8961e-04,\n",
      "          1.1408e-01,  1.4276e-01, -2.1193e-04, -2.8272e-01, -2.8552e-02,\n",
      "         -1.3925e-01],\n",
      "        [ 2.3386e-01, -2.2633e-01,  6.7598e-02, -5.7542e-01,  4.7715e-04,\n",
      "         -1.1118e-01, -1.3912e-01,  2.0653e-04,  2.7552e-01,  2.7825e-02,\n",
      "          1.3570e-01],\n",
      "        [-1.4161e-01,  1.3705e-01, -4.0932e-02,  3.4843e-01, -2.8893e-04,\n",
      "          6.7320e-02,  8.4242e-02, -1.2506e-04, -1.6683e-01, -1.6849e-02,\n",
      "         -8.2172e-02],\n",
      "        [ 2.4301e-01, -2.3519e-01,  7.0244e-02, -5.9795e-01,  4.9583e-04,\n",
      "         -1.1553e-01, -1.4457e-01,  2.1462e-04,  2.8630e-01,  2.8915e-02,\n",
      "          1.4102e-01],\n",
      "        [ 4.1364e-02, -4.0033e-02,  1.1957e-02, -1.0178e-01,  8.4397e-05,\n",
      "         -1.9665e-02, -2.4608e-02,  3.6531e-05,  4.8733e-02,  4.9217e-03,\n",
      "          2.4003e-02],\n",
      "        [-1.9155e-01,  1.8538e-01, -5.5368e-02,  4.7131e-01, -3.9082e-04,\n",
      "          9.1061e-02,  1.1395e-01, -1.6917e-04, -2.2567e-01, -2.2791e-02,\n",
      "         -1.1115e-01],\n",
      "        [ 2.6659e-01, -2.5801e-01,  7.7059e-02, -6.5595e-01,  5.4393e-04,\n",
      "         -1.2673e-01, -1.5859e-01,  2.3544e-04,  3.1408e-01,  3.1720e-02,\n",
      "          1.5470e-01],\n",
      "        [-1.7321e-01,  1.6764e-01, -5.0068e-02,  4.2620e-01, -3.5341e-04,\n",
      "          8.2344e-02,  1.0304e-01, -1.5297e-04, -2.0407e-01, -2.0609e-02,\n",
      "         -1.0051e-01],\n",
      "        [ 1.2990e-01, -1.2572e-01,  3.7549e-02, -3.1963e-01,  2.6504e-04,\n",
      "         -6.1755e-02, -7.7279e-02,  1.1472e-04,  1.5304e-01,  1.5456e-02,\n",
      "          7.5380e-02],\n",
      "        [ 4.9881e-01, -4.8276e-01,  1.4418e-01, -1.2274e+00,  1.0177e-03,\n",
      "         -2.3713e-01, -2.9674e-01,  4.4053e-04,  5.8767e-01,  5.9351e-02,\n",
      "          2.8945e-01],\n",
      "        [ 3.9059e-01, -3.7802e-01,  1.1290e-01, -9.6106e-01,  7.9692e-04,\n",
      "         -1.8568e-01, -2.3236e-01,  3.4495e-04,  4.6017e-01,  4.6473e-02,\n",
      "          2.2665e-01],\n",
      "        [ 2.0133e-01, -1.9485e-01,  5.8196e-02, -4.9539e-01,  4.1078e-04,\n",
      "         -9.5713e-02, -1.1977e-01,  1.7781e-04,  2.3720e-01,  2.3955e-02,\n",
      "          1.1683e-01],\n",
      "        [-1.4149e-01,  1.3694e-01, -4.0898e-02,  3.4814e-01, -2.8868e-04,\n",
      "          6.7263e-02,  8.4171e-02, -1.2496e-04, -1.6669e-01, -1.6835e-02,\n",
      "         -8.2103e-02],\n",
      "        [ 1.8803e-01, -1.8198e-01,  5.4352e-02, -4.6266e-01,  3.8365e-04,\n",
      "         -8.9390e-02, -1.1186e-01,  1.6606e-04,  2.2153e-01,  2.2373e-02,\n",
      "          1.0911e-01],\n",
      "        [-1.4267e-01,  1.3808e-01, -4.1239e-02,  3.5104e-01, -2.9109e-04,\n",
      "          6.7823e-02,  8.4872e-02, -1.2600e-04, -1.6808e-01, -1.6975e-02,\n",
      "         -8.2787e-02],\n",
      "        [-3.3319e-01,  3.2247e-01, -9.6311e-02,  8.1984e-01, -6.7982e-04,\n",
      "          1.5840e-01,  1.9822e-01, -2.9426e-04, -3.9255e-01, -3.9644e-02,\n",
      "         -1.9335e-01],\n",
      "        [-2.1132e-02,  2.0452e-02, -6.1082e-03,  5.1996e-02, -4.3116e-05,\n",
      "          1.0046e-02,  1.2571e-02, -1.8663e-05, -2.4896e-02, -2.5143e-03,\n",
      "         -1.2262e-02],\n",
      "        [ 6.9836e-02, -6.7589e-02,  2.0187e-02, -1.7184e-01,  1.4249e-04,\n",
      "         -3.3200e-02, -4.1546e-02,  6.1676e-05,  8.2277e-02,  8.3094e-03,\n",
      "          4.0525e-02],\n",
      "        [-3.5247e-02,  3.4112e-02, -1.0188e-02,  8.6726e-02, -7.1915e-05,\n",
      "          1.6756e-02,  2.0968e-02, -3.1128e-05, -4.1525e-02, -4.1938e-03,\n",
      "         -2.0453e-02],\n",
      "        [ 1.2414e-01, -1.2014e-01,  3.5883e-02, -3.0545e-01,  2.5329e-04,\n",
      "         -5.9016e-02, -7.3851e-02,  1.0963e-04,  1.4625e-01,  1.4771e-02,\n",
      "          7.2036e-02],\n",
      "        [ 3.8093e-02, -3.6867e-02,  1.1011e-02, -9.3730e-02,  7.7722e-05,\n",
      "         -1.8109e-02, -2.2661e-02,  3.3642e-05,  4.4879e-02,  4.5324e-03,\n",
      "          2.2105e-02],\n",
      "        [ 6.0212e-02, -5.8274e-02,  1.7404e-02, -1.4815e-01,  1.2285e-04,\n",
      "         -2.8624e-02, -3.5820e-02,  5.3176e-05,  7.0938e-02,  7.1642e-03,\n",
      "          3.4940e-02],\n",
      "        [-1.3732e-01,  1.3290e-01, -3.9694e-02,  3.3789e-01, -2.8018e-04,\n",
      "          6.5283e-02,  8.1694e-02, -1.2128e-04, -1.6179e-01, -1.6339e-02,\n",
      "         -7.9686e-02],\n",
      "        [-2.8204e-01,  2.7297e-01, -8.1526e-02,  6.9398e-01, -5.7546e-04,\n",
      "          1.3408e-01,  1.6779e-01, -2.4909e-04, -3.3229e-01, -3.3559e-02,\n",
      "         -1.6366e-01],\n",
      "        [ 2.7656e-02, -2.6766e-02,  7.9940e-03, -6.8048e-02,  5.6427e-05,\n",
      "         -1.3147e-02, -1.6452e-02,  2.4424e-05,  3.2582e-02,  3.2906e-03,\n",
      "          1.6048e-02],\n",
      "        [ 1.7114e-01, -1.6564e-01,  4.9470e-02, -4.2111e-01,  3.4919e-04,\n",
      "         -8.1362e-02, -1.0181e-01,  1.5115e-04,  2.0163e-01,  2.0363e-02,\n",
      "          9.9312e-02]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad.zero_()\n",
    "x_aux = Variable(x_test, requires_grad = True)\n",
    "cost_defender = learner_cost(w, b, x_aux, y_test, 0.0)\n",
    "cost_defender.backward()\n",
    "print(w.grad) ### This is the grad wrt w, the ith element is first summand above of the ith term\n",
    "print(x_aux.grad) ## The ith row is the first part of the second summand above, of the ith element\n",
    "x_aux.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jacobian(inputs, outputs):\n",
    "    return torch.stack([grad([outputs[:, i].sum()], [inputs], retain_graph=True, create_graph=True)[0]\n",
    "                        for i in range(outputs.size(1))], dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.3646e-01, -1.3400e-02,  4.0022e-03, -3.4068e-02,  2.8250e-05,\n",
       "         -6.5822e-03, -8.2368e-03,  1.2228e-05,  1.6312e-02,  1.6474e-03,\n",
       "          8.0344e-03],\n",
       "        [-9.5354e-03, -5.4107e-01, -2.7563e-03,  2.3462e-02, -1.9455e-05,\n",
       "          4.5331e-03,  5.6726e-03, -8.4213e-06, -1.1234e-02, -1.1346e-03,\n",
       "         -5.5332e-03],\n",
       "        [ 2.3728e-02, -2.2965e-02, -5.4344e-01, -5.8385e-02,  4.8413e-05,\n",
       "         -1.1280e-02, -1.4116e-02,  2.0956e-05,  2.7955e-02,  2.8233e-03,\n",
       "          1.3769e-02],\n",
       "        [-1.1740e-02,  1.1362e-02, -3.3936e-03, -5.2142e-01, -2.3954e-05,\n",
       "          5.5813e-03,  6.9842e-03, -1.0368e-05, -1.3832e-02, -1.3969e-03,\n",
       "         -6.8126e-03],\n",
       "        [ 3.0358e-03, -2.9381e-03,  8.7751e-04, -7.4697e-03, -5.5030e-01,\n",
       "         -1.4432e-03, -1.8060e-03,  2.6811e-06,  3.5766e-03,  3.6121e-04,\n",
       "          1.7616e-03],\n",
       "        [-1.1209e-02,  1.0848e-02, -3.2400e-03,  2.7581e-02, -2.2870e-05,\n",
       "         -5.4497e-01,  6.6683e-03, -9.8993e-06, -1.3206e-02, -1.3337e-03,\n",
       "         -6.5044e-03],\n",
       "        [-3.0320e-03,  2.9345e-03, -8.7643e-04,  7.4605e-03, -6.1863e-06,\n",
       "          1.4414e-03, -5.4850e-01, -2.6778e-06, -3.5722e-03, -3.6076e-04,\n",
       "         -1.7594e-03],\n",
       "        [-4.1567e-03,  4.0230e-03, -1.2015e-03,  1.0228e-02, -8.4811e-06,\n",
       "          1.9761e-03,  2.4728e-03, -5.5031e-01, -4.8972e-03, -4.9458e-04,\n",
       "         -2.4121e-03],\n",
       "        [-7.8995e-03,  7.6452e-03, -2.2834e-03,  1.9437e-02, -1.6117e-05,\n",
       "          3.7554e-03,  4.6994e-03, -6.9764e-06, -5.5961e-01, -9.3990e-04,\n",
       "         -4.5839e-03],\n",
       "        [-1.1039e-03,  1.0684e-03, -3.1910e-04,  2.7163e-03, -2.2524e-06,\n",
       "          5.2481e-04,  6.5673e-04, -9.7494e-07, -1.3006e-03, -5.5043e-01,\n",
       "         -6.4059e-04],\n",
       "        [-1.3756e-02,  1.3313e-02, -3.9762e-03,  3.3847e-02, -2.8067e-05,\n",
       "          6.5395e-03,  8.1834e-03, -1.2149e-05, -1.6206e-02, -1.6367e-03,\n",
       "         -5.5829e-01]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = 0.1\n",
    "c_d = torch.ones(len(y_train))*value\n",
    "z = torch.zeros([len(y_train),1]) \n",
    "\n",
    "out = attack(w, x_train, c_d, z)\n",
    "\n",
    "## The i,j element of this is the derivative of the 0,j element of the transformation wrt w_i\n",
    "## The ith row is the derivative of phi(0) wrt w_i\n",
    "w.grad.zero_()\n",
    "jacobian(w, out[0].reshape(1,-1))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Defense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def opt_defense(x, y, w, c_d, z):\n",
    "    out = attack(w, x, c_d, z)\n",
    "    m1 = torch.eye(out.shape[1]) + out.t() @ out\n",
    "    inv = torch.inverse( m1 )\n",
    "    last = out.t() @ y\n",
    "    w_opt = inv @ last\n",
    "    return(w_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_opt = opt_defense(x_train, y_train, w, c_d, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_after_defense = x_train @ w_opt + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7312)"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt( torch.mean( (pred_after_defense - y_train )**2 ) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.random.randn(10)\n",
    "b = np.random.randn(10)\n",
    "df = pd.DataFrame({\"first\":range(10), \"sec\":b})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"lele\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
