{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import grad\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def redim(x):\n",
    "    return(np.append(x, np.ones([x.shape[0],1]), axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stand(x, mean, std):\n",
    "    x = x - mean\n",
    "    x = x/std \n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        6  \n",
       "1      9.5        6  \n",
       "2     10.1        6  \n",
       "3      9.9        6  \n",
       "4      9.9        6  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/winequality-white.csv\", sep = \";\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data.loc[:, data.columns != \"quality\"]\n",
    "y = data.quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract first few PCA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.09657344e-01 7.93338631e-02 1.01542742e-02 5.06004450e-04\n",
      " 3.23409395e-04 8.72769740e-06 6.72986618e-06 5.39060918e-06\n",
      " 4.07002123e-06 1.86525322e-07 1.49217279e-10]\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=11, svd_solver='full')\n",
    "pca.fit(X)                 \n",
    "print(pca.explained_variance_ratio_) \n",
    "x = pca.fit_transform(X)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
    "x_train = np.asarray(x_train,dtype=np.float32)\n",
    "y_train = np.asarray(y_train,dtype=np.float32).reshape(-1,1)\n",
    "x_test = np.asarray(x_test,dtype=np.float32) #un poco trampa\n",
    "y_test = np.asarray(y_test,dtype=np.float32).reshape(-1,1)\n",
    "### to torch\n",
    "x_train = Variable( torch.from_numpy(x_train) )\n",
    "y_train = Variable( torch.from_numpy(y_train) )\n",
    "x_test = torch.from_numpy(x_test) \n",
    "y_test = torch.from_numpy(y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_renom = stand(x_train, x_train.mean(dim=0), x_train.std(dim=0))\n",
    "x_test_renom = stand(x_test, x_train.mean(dim=0), x_train.std(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using torch\n",
    "\n",
    "Convention: last weight will be the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = torch.randn(1, x_train.shape[1] + 1, requires_grad=True)\n",
    "lmb = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(x, w):\n",
    "    weights = w[0,:-1].view(1,-1)\n",
    "    bias = w[0,-1]\n",
    "    return( x @ weights.t() + bias )\n",
    "\n",
    "def mse(t1, t2, w):\n",
    "    diff = t1 - t2\n",
    "    return( torch.sum(diff * diff) / diff.numel() + lmb*w @ w.t() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1000, loss tensor([ 0.5743])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    epoch += 1\n",
    "    preds = model(x_train_renom, w)\n",
    "    loss = mse(preds, y_train, w)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * lr\n",
    "        w.grad.zero_()\n",
    "        \n",
    "    if epoch%1000== 0:\n",
    "        print('epoch {}, loss {}'.format(epoch,loss.data[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1249,  0.1581, -0.0413,  0.3557,  0.0124,  0.0770,  0.0880,\n",
      "         -0.0032, -0.1719, -0.0307, -0.0728,  5.8818]])\n"
     ]
    }
   ],
   "source": [
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse(y, pred):\n",
    "    return torch.sqrt( torch.mean( (pred - y )**2 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7360)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(x_test_renom, w)\n",
    "rmse(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial attack. Non-Bayesian case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X$ denote the clean dataset, and $X^* = T(X, \\beta)$ the attacked dataset, when the classifier choose parameters $\\beta$. We try to solve the following Defend-Attack game\n",
    "\n",
    "$$\n",
    "\\beta^* = \\arg\\min_{\\beta} \\widehat{\\theta}_C [\\beta, T(X, \\beta)] = \\arg\\min_{\\beta} \\sum_{i=1}^n \\left( T(x, \\beta)^{[i]}\\beta^{\\top} - y_i \\right)^2 + \\lambda \\beta \\beta^{\\top}\n",
    "$$\n",
    "\n",
    "subject to\n",
    "\n",
    "$$\n",
    "X^* = T(X, \\beta) = \\arg\\min_{X'} \\widehat{\\theta}_A [\\beta, X'] = \\arg\\min_{X'} \\sum_{i=1}^n c_{i}\\left( X'^{[i]}\\beta^{\\top} - z_i \\right)^2 + ||X-X'||^2_{F}\n",
    "$$\n",
    "\n",
    "Where $y$ are the true labels, $z$ are the targets and $c$ are instance-specific factors, which are common knowledge here. We can solve exactly the attacker's problem, yielding\n",
    "\n",
    "$$\n",
    "X^* = T(X, \\beta) = X - \\left(\\text{diag}(c_d)^{-1} + \\beta \\beta^{\\top} I_n \\right)^{-1} (X\\beta - z)\\beta^\\top\n",
    "$$\n",
    "\n",
    "We could then compute the gradient for the classifier problem using\n",
    "\n",
    "$$\n",
    "\\nabla \\widehat{\\theta}_C [\\beta, T(X, \\beta)] = \\nabla_{\\beta} \\widehat{\\theta}_C [\\beta, T(X, \\beta)] + \\nabla_T  \\widehat{\\theta}_C [\\beta, T(X, \\beta)] \\frac{\\partial T(X,\\beta)}{\\partial \\beta}\n",
    "$$\n",
    "\n",
    "and use gradient descent to find $\\beta^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defense - Forward mode\n",
    "\n",
    "### Attack - Analytic form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exact solution to the attacker problem\n",
    "def attack(w, instance, c_d, z):\n",
    "    weights = w[0,:-1].view(1,-1)\n",
    "    bias = w[0,-1]\n",
    "    ##\n",
    "    p1 = ( 1/c_d + weights @ weights.t() )**(-1)\n",
    "    p1 = torch.diag( p1.squeeze(1) )\n",
    "    p2 = ( instance @ weights.t() - (z - bias) ) @ weights\n",
    "    out = instance - p1 @ p2\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "value = 0.5 ## Same c_i for every instance\n",
    "c_d = torch.ones([len(y_test), 1])*value\n",
    "z = torch.zeros([len(y_test),1]) \n",
    "out = attack(w, x_test_renom, c_d, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean test RMSE:  tensor(0.7360)\n",
      "Attacked est RMSE:  tensor(0.9263)\n"
     ]
    }
   ],
   "source": [
    "pred_at =  model(out, w)\n",
    "pred_clean = model(x_test_renom, w)\n",
    "print(\"Clean test RMSE: \", torch.sqrt( torch.mean( (pred_clean - y_test )**2 ) ) )\n",
    "print(\"Attacked est RMSE: \", torch.sqrt( torch.mean( (pred_at- y_test )**2 ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attack - Using torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 10e-2\n",
    "epochs = 100\n",
    "value = 0.5\n",
    "#\n",
    "c_d = torch.ones([len(y_test), 1])*value\n",
    "z = torch.zeros([len(y_test),1]) \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attacker_cost_flatten(w, x, x_old, c_d, z):\n",
    "    weights = w[0,:-1].view(1,-1)\n",
    "    bias = w[0,-1]\n",
    "    x = x.view(x_old.shape[0],-1)\n",
    "    ##\n",
    "    diff = x_old - x\n",
    "    return  torch.sum( c_d*(x @ weights.t() + bias)**2 )  +  torch.sum(diff**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, loss 23466.92578125\n",
      "epoch 20, loss 22885.5625\n",
      "epoch 30, loss 22879.201171875\n",
      "epoch 40, loss 22879.130859375\n",
      "epoch 50, loss 22879.12890625\n",
      "epoch 60, loss 22879.12890625\n",
      "epoch 70, loss 22879.12890625\n",
      "epoch 80, loss 22879.12890625\n",
      "epoch 90, loss 22879.12890625\n",
      "epoch 100, loss 22879.12890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel/__main__.py:15: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    }
   ],
   "source": [
    "instance = x_test_renom\n",
    "out = attack(w, instance, c_d, z)\n",
    "attacked_instance = torch.randn(x_test_renom.shape[0]*x_test.shape[1], requires_grad=True)\n",
    "##\n",
    "for epoch in range(epochs):\n",
    "    epoch += 1\n",
    "    loss = attacker_cost_flatten(w, attacked_instance, instance, c_d, z) \n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        attacked_instance -= attacked_instance.grad * lr\n",
    "        attacked_instance.grad.zero_()\n",
    "\n",
    "        \n",
    "    if epoch%10 == 0:\n",
    "        print('epoch {}, loss {}'.format(epoch,loss.data[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(22879.1289)\n",
      "tensor(22879.1289)\n"
     ]
    }
   ],
   "source": [
    "print(attacker_cost_flatten(w, attacked_instance, instance, c_d, z))\n",
    "print(attacker_cost_flatten(w, out.view(-1,1), instance, c_d, z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learner_cost_flatten(w, x, y, lmb):\n",
    "    x = x.view(-1,w.shape[1]-1) \n",
    "    weights = w[0,:-1].view(1,-1)\n",
    "    bias = w[0,-1]\n",
    "    return torch.sum( (x @ weights.t() + bias - y)**2 ) +  lmb * weights @ weights.t()\n",
    "\n",
    "def attacker_cost_flatten(w, x, x_old, c_d, z):\n",
    "    weights = w[0,:-1].view(1,-1)\n",
    "    bias = w[0,-1]\n",
    "    x = x.view(x_old.shape[0],-1)\n",
    "    ##\n",
    "    diff = x_old - x\n",
    "    return  torch.sum( c_d*(x @ weights.t() + bias)**2 )  +  torch.sum(diff**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defense Forward Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "def compute_full_second_derivative(vec_func, var):\n",
    "    tmp = torch.zeros( int(np.max(var.shape)), vec_func.shape[0])\n",
    "    for i, loss in enumerate(vec_func):\n",
    "        tmp[:,i] = torch.autograd.grad(loss, var, retain_graph=True)[0]\n",
    "    return tmp\n",
    "\n",
    "##\n",
    "def do_forward_multidim(w, x, x_clean, c_d, z, y_train, lmb, T=100):\n",
    "    lr = 10e-6 # Outer learning rate\n",
    "    ilr = 0.01 # Inner learning rate\n",
    "    ##\n",
    "    gm = lambda w, x: attacker_cost_flatten(w, x, x_clean, c_d, z)\n",
    "    fm = lambda w, x: learner_cost_flatten(w, x, y_train, lmb)\n",
    "    ##\n",
    "    Z = torch.zeros(x.shape[0], w.shape[1]) \n",
    "\n",
    "\n",
    "    for i in range(T):\n",
    "        # We nee to compute the total derivative of f wrt x\n",
    "        #y = 0.0\n",
    "\n",
    "        for j in range(T):\n",
    "            grad_x = torch.autograd.grad(gm(w,x), x, create_graph=True)[0]\n",
    "            new_x = x - ilr*grad_x\n",
    "            ##\n",
    "            A_tensor = compute_full_second_derivative(new_x, x)\n",
    "            B_tensor = compute_full_second_derivative(new_x, w)\n",
    "            ##\n",
    "            Z = A_tensor @ Z + B_tensor.t()\n",
    "            #Z = Z @ A_tensor + B_tensor\n",
    "            x = Variable(new_x, requires_grad=True)\n",
    "\n",
    "        grad_w = torch.autograd.grad(fm(w,x), w, retain_graph=True)[0] \n",
    "        grad_x = torch.autograd.grad(fm(w,x), x)[0]\n",
    "        ##\n",
    "        # print(grad_x.shape, Z.shape, grad_w.shape)\n",
    "        w = w - lr*(grad_w + grad_x @ Z)\n",
    "        print(fm(w,x))\n",
    "    return(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-cdc4b920c52d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train_renom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mattacked_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_renom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx_train_renom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mw_clean_fw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_forward_multidim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_clean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattacked_instance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-10de01942ad1>\u001b[0m in \u001b[0;36mdo_forward_multidim\u001b[0;34m(w, x, x_clean, c_d, z, y_train, lmb, T)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mA_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_full_second_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mB_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_full_second_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;31m##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA_tensor\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mZ\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mB_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-10de01942ad1>\u001b[0m in \u001b[0;36mcompute_full_second_derivative\u001b[0;34m(vec_func, var)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    142\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    143\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         inputs, allow_unused)\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "value = 0.5\n",
    "c_d = torch.ones([len(y_train), 1])*value\n",
    "z = torch.zeros([len(y_train),1]) \n",
    "w_clean = torch.randn(1, x_train.shape[1] + 1, requires_grad=True)\n",
    "instance = x_train_renom\n",
    "attacked_instance = torch.randn(x_train_renom.shape[0]*x_train_renom.shape[1], requires_grad=True)\n",
    "w_clean_fw = do_forward_multidim(w_clean, attacked_instance, instance, c_d, z, y_train, lmb=0.0, T=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defense Backward Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_backward_multidim(w, x, x_clean, c_d, z, y_train, lmb, T=100):\n",
    "    lr = 10e-6 # Outer learning rate\n",
    "    ilr = 0.01 # Inner learning rate\n",
    "    ##\n",
    "    gm = lambda w, x: attacker_cost_flatten(w, x, x_clean, c_d, z)\n",
    "    fm = lambda w, x: learner_cost_flatten(w, x, y_train, lmb)\n",
    "    ##\n",
    "    xt = torch.zeros(int(T), x.shape[0]) \n",
    "\n",
    "    for i in range(T):\n",
    "        # We nee to compute the total derivative of f wrt x\n",
    "        ##    \n",
    "        for j in range(T):\n",
    "            grad_x = torch.autograd.grad(gm(w,x), x, create_graph=True)[0]\n",
    "            new_x = x - ilr*grad_x\n",
    "            x = Variable(new_x, requires_grad=True)\n",
    "            xt[j] = x\n",
    "        ## CHECK WITH ANALYTICAL SOLUTION\n",
    "        ###\n",
    "        alpha = -torch.autograd.grad(fm(w,x), x, retain_graph=True)[0]\n",
    "        gr = torch.zeros_like(w)\n",
    "        ###\n",
    "        for j in range(T-1,-1,-1):\n",
    "            x_tmp = Variable(xt[j], requires_grad=True)\n",
    "            grad_x, = torch.autograd.grad( gm(w,x_tmp), x_tmp, create_graph=True )\n",
    "            loss = x_tmp - ilr*grad_x\n",
    "            loss = loss@alpha\n",
    "            aux1 = torch.autograd.grad(loss, w, retain_graph=True)[0]\n",
    "            aux2 = torch.autograd.grad(loss, x_tmp)[0]\n",
    "            gr -= aux1\n",
    "            alpha = aux2 \n",
    "\n",
    "        grad_w = torch.autograd.grad(fm(w,x), w)[0] \n",
    "        ##\n",
    "        w = w - lr*(grad_w + gr)\n",
    "        \n",
    "        if i%10 == 0:\n",
    "            print( 'epoch {}, loss {}'.format(i,fm(w,x)) )\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = 0.5\n",
    "c_d = torch.ones([len(y_train), 1])*value\n",
    "z = torch.zeros([len(y_train),1]) \n",
    "w_clean = torch.randn(1, x_train.shape[1] + 1, requires_grad=True)\n",
    "instance = x_train_renom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss tensor([[nan.]])\n"
     ]
    }
   ],
   "source": [
    "attacked_instance = torch.randn(x_train_renom.shape[0]*x_train_renom.shape[1], requires_grad=True)\n",
    "w_clean_bw = do_backward_multidim(w_clean, attacked_instance, instance, c_d, z, y_train, lmb=0.0, T=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_clean_bw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Nash Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.4470)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = 0.5\n",
    "c_d = torch.ones([len(y_test), 1])*value\n",
    "z = torch.zeros([len(y_test),1]) \n",
    "##\n",
    "out = attack(w, x_test_renom, c_d, z)\n",
    "preds = model(out, w_clean)\n",
    "rmse(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9263)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(out, w)\n",
    "rmse(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defense Analytical Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_a = w[0][:-1]\n",
    "b_a = w[0][-1]\n",
    "##\n",
    "w_nash = torch.randn(1, x_train.shape[1], requires_grad=True)\n",
    "b_nash = torch.randn(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attack_a(w, b, test, c_d, z):\n",
    "    c_d = ( 1/c_d + w @ w.t() )**(-1)\n",
    "    p1 = torch.diag( c_d[0] )\n",
    "    #p1 = torch.inverse( torch.inverse( torch.diag(c_d) ) +  w @ w.t() * torch.eye( test.shape[0] ) )\n",
    "    p2 = ( test @ w.t() + b - z)@w \n",
    "    out = test - p1 @ p2\n",
    "    return(out)\n",
    "\n",
    "def learner_cost_a(w, b, x, y, lmb, c_d, z):\n",
    "    out = attack_a(w, b, x, c_d, z)\n",
    "    #out = stand(out, out.mean(dim=0), out.std(dim=0))\n",
    "    #print(out.std(dim=0))\n",
    "    return torch.sum( (out @ w.t() + b - y)**2 ) +  lmb * w @ w.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 10e-6\n",
    "epochs = 400\n",
    "value = 0.5\n",
    "c_d = torch.ones(len(y_train))*value\n",
    "z = torch.zeros([len(y_train),1]) \n",
    "print(\"Initial Cost\", learner_cost(w_nash, b_nash, x_train_renom, y_train, lmb, c_d, z))\n",
    "for epoch in range(epochs):\n",
    "    epoch += 1\n",
    "    loss = learner_cost(w_nash, b_nash, x_train_renom, y_train, lmb, c_d, z)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w_nash -= w_nash.grad * lr\n",
    "        b_nash -= b_nash.grad * lr\n",
    "        w_nash.grad.zero_()\n",
    "        b_nash.grad.zero_()\n",
    "        \n",
    "    if epoch%100 == 0:\n",
    "        print('epoch {}, loss {}'.format(epoch,loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w_nash)\n",
    "print(b_nash)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_clean_bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
